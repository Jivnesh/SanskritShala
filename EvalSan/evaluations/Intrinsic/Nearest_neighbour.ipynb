{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cosine_similarity(word1, word2):\n",
    "    # word1: [N, sum(Filters)]\n",
    "    # word2: [voca, sum(filters)]\n",
    "    word1 = np.array(word1)\n",
    "    word2 = np.array(word2)\n",
    "    dot = np.dot(word1, word2.T) # [N, voca]\n",
    "    word1_size = np.sqrt(np.sum(np.square(word1), axis=-1)) # [N]\n",
    "    word2_size = np.sqrt(np.sum(np.square(word2), axis=-1)) # [voca]\n",
    "    size = np.multiply(word1_size.reshape(-1, 1), word2_size) # [N, voca]\n",
    "    cosim = dot/size # [N, voca]\n",
    "    return cosim # [N, voca]\n",
    "\n",
    "def _top_k_cosine_similarity(current_word_embedding, voca_embedding, embedding_name, word, top_k=3, name=None):\n",
    "    cosim = _cosine_similarity(current_word_embedding, voca_embedding)\n",
    "#     print(cosim.shape)\n",
    "    argsort = np.argsort(-cosim)[:, :top_k] # decreasing order\n",
    "    NN = []\n",
    "    for row in argsort:\n",
    "        temp = []\n",
    "        for col in row:\n",
    "            temp.append(embedding_name[int(col)])\n",
    "        NN.append(temp)\n",
    "        \n",
    "    top_k_cosim = np.array([cosim[index][i] for index, i in enumerate(argsort)]) # [N, top_k]\n",
    "\n",
    "    print(name)\n",
    "    for i in range(len(word)):\n",
    "        print('input_word:', word[i])\n",
    "        print(np.array(list(zip(NN[i], top_k_cosim[i]))))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Load the library\n",
    "import fasttext\n",
    "import json\n",
    "model = fasttext.load_model(\"../../models/FastText/result/corpus_word_continuous.txt.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emb_dict ={}\n",
    "with open('./vec_files/FastText.300.vec', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word, vec = line.split(' ', 1)\n",
    "        emb_dict[word] = np.fromstring(vec, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_highway\n",
      "input_word: tAByAm\n",
      "[['tAByAm' '1.000000056177758']\n",
      " ['tayoH' '0.49548605706870197']\n",
      " ['tO' '0.4714334801003086']\n",
      " ['dvArO' '0.44562153380335845']\n",
      " ['iti-etAByAm' '0.4443622636209815']\n",
      " ['cakzuqByAm' '0.4076752226616034']\n",
      " ['BUByAm' '0.40690532204182556']\n",
      " ['tatas-tAByAm' '0.40588611021396054']]\n",
      "\n",
      "input_word: ahham\n",
      "[['siṁham' '0.4546009509466571']\n",
      " ['tam' '0.42580659124516135']\n",
      " ['tat' '0.4109035211604918']\n",
      " ['yaTA-aham' '0.395642486824646']\n",
      " ['vaṁSam' '0.3942224525806031']\n",
      " ['sa-asram' '0.39024018014114414']\n",
      " ['kaṁsam' '0.3888144790848053']\n",
      " ['sarvam' '0.3857804238939817']]\n",
      "\n",
      "input_word: bahUhuni\n",
      "[['bahuni' '0.6083027695450411']\n",
      " ['mandmatyoH' '0.5466141793526074']\n",
      " ['suprayogaviSiKa' '0.5306529275065406']\n",
      " ['hata-Adara-tayA' '0.5275621022344397']\n",
      " ['sa-saNgam-asaNgam-iti' '0.5232818076813854']\n",
      " ['bahavaH-tatra' '0.5193559600618802']\n",
      " ['deSa-antaram' '0.5177594114726974']\n",
      " ['pratiniDyoH' '0.5106806803959424']]\n",
      "\n",
      "input_word: sahasra-cakzo\n",
      "[['sahasraxaMRtra' '0.76442225247277']\n",
      " ['sahasra-kftvas-' '0.7593064232245955']\n",
      " ['sahasra-veDI' '0.7559249261807793']\n",
      " ['sahasra-ekam' '0.7490438170422113']\n",
      " ['sahasra-SIrzA' '0.7445367284565849']\n",
      " ['sahasra-Sirase' '0.7395297362852099']\n",
      " ['sahasrapawra' '0.7360528309015999']\n",
      " ['sahasra-aMSena' '0.7285034457370445']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "word = ['BAratasya','suKam','satyam','nityam','aham','AByAm','ahham','bahUhuni','sahasra-cakzo']\n",
    "# word = ['drAvyate','saMgrasate','ayuDyati','pAtaye','dravyAn']\n",
    "word = []\n",
    "embed_space = []\n",
    "for w in emb_dict.keys():\n",
    "    embed_space.append(emb_dict[w])\n",
    "for w in word:\n",
    "    vectors.append(model.get_word_vector(w))\n",
    "_top_k_cosine_similarity(vectors, embed_space, list(emb_dict.keys()), word, top_k=8, name='after_highway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  open('./Data/embedding_space_or_vocab.txt','r')\n",
    "lines = f.readlines()\n",
    "voca = set()\n",
    "for line in lines:\n",
    "    line = line.replace('\\n','')\n",
    "    voca.add(line)\n",
    "f.close()\n",
    "voca.remove('')\n",
    "voca = list(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_highway\n",
      "input_word: tAByAm\n",
      "[['tAByAm' '0.9999999999999992']\n",
      " ['AvAByAm' '0.3375851360269416']\n",
      " ['dvAByAm' '0.32601343729616217']\n",
      " ['tayA' '0.3079213692836734']\n",
      " ['mayA' '0.3038272780186519']]\n",
      "\n",
      "input_word: ahham\n",
      "[['allAh' '0.7193251074620278']\n",
      " ['ahne' '0.7128906473693825']\n",
      " ['ahrasat' '0.7015612728601706']\n",
      " ['ahrasan' '0.698943611739984']\n",
      " ['ahrasam' '0.6938093441899283']]\n",
      "\n",
      "input_word: bahUhuni\n",
      "[['mAnini' '0.6013870385686777']\n",
      " ['sani' '0.5880590737609316']\n",
      " ['dahu' '0.5775882315337663']\n",
      " ['cUrRitAni' '0.5710922752912875']\n",
      " ['nivizwAni' '0.5709430918875563']]\n",
      "\n",
      "input_word: sahasra-cakzo\n",
      "[['pracakzva' '0.6249167257292929']\n",
      " ['suparRAH-ca' '0.582374904384294']\n",
      " ['pracakzmahe' '0.5613083197245341']\n",
      " ['vAlaKilyAH-ca' '0.5521646846738717']\n",
      " ['antarikzAt-' '0.5503355553623744']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import sentencepiece as spm\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.load('../../models/corpus_variants/Sentencepiece_Model'+'/model_vocab_32000.model')\n",
    "oov = 0\n",
    "vectors = []\n",
    "words = word\n",
    "# words = ['BAratasya','suKam','satyam']\n",
    "saved_model = gensim.models.Word2Vec.load('../../models/word2vec/saved_models/word2vec.model')\n",
    "embed_space = []\n",
    "\n",
    "for w in voca:\n",
    "    enc = sp_model.encode_as_pieces(w)\n",
    "    vector = np.zeros(300)\n",
    "    piece_size = 0\n",
    "    for piece in enc:\n",
    "        if piece in saved_model.wv.vocab:\n",
    "            vector += saved_model.wv[piece]\n",
    "            piece_size += 1\n",
    "\n",
    "    if piece_size != 0:\n",
    "        vector = vector/piece_size\n",
    "    else:\n",
    "        vector =  np.random.rand(300)\n",
    "        oov += 1\n",
    "    embed_space.append(vector)\n",
    "    \n",
    "for word in words:\n",
    "    enc = sp_model.encode_as_pieces(word)\n",
    "    vector = np.zeros(300)\n",
    "    piece_size = 0\n",
    "    for piece in enc:\n",
    "        if piece in saved_model.wv.vocab:\n",
    "            vector += saved_model.wv[piece]\n",
    "            piece_size += 1\n",
    "\n",
    "    if piece_size != 0:\n",
    "        vector = vector/piece_size\n",
    "    else:\n",
    "        vector =  np.random.rand(300)\n",
    "        oov += 1\n",
    "    vectors.append(vector)\n",
    "_top_k_cosine_similarity(vectors, embed_space, voca , words, top_k=5, name='after_highway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_highway\n",
      "input_word: tAByAm\n",
      "[['tAByAm' '0.9999999999999999']\n",
      " ['aDikAriByAm' '0.34620537734488294']\n",
      " ['pArzRiByAm' '0.33669497098130713']\n",
      " ['jaNGAByAm' '0.3216142465375557']\n",
      " ['tO' '0.31422354429472865']]\n",
      "\n",
      "input_word: ahham\n",
      "[['allAh' '0.7606563966192078']\n",
      " ['ahne' '0.7435124235905444']\n",
      " ['ahnuvi' '0.6955578327008431']\n",
      " ['ahnUye' '0.6891296525573866']\n",
      " ['ahnUyAvahi' '0.6869392062943536']]\n",
      "\n",
      "input_word: bahUhuni\n",
      "[['bahU' '0.5814050688005236']\n",
      " ['behulA' '0.539564070142775']\n",
      " ['behulAyAH' '0.5068954393817564']\n",
      " ['dahu' '0.5051067533720685']\n",
      " ['venitvA' '0.5035352385266239']]\n",
      "\n",
      "input_word: sahasra-cakzo\n",
      "[['tat-ca-eva' '0.7604227190331986']\n",
      " ['taTA-ca' '0.7599812090442553']\n",
      " ['yat-ca-eva' '0.7582624109981531']\n",
      " ['nicayAt-' '0.7579032428331048']\n",
      " ['tat-ca-api' '0.7578166516676037']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_dict = {}\n",
    "oov = 0\n",
    "embed_space = []\n",
    "vectors = []\n",
    "with open('../../models/GloVe/saved_models/vectors.txt', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word, vec = line.split(' ', 1)\n",
    "        emb_dict[word] = np.fromstring(vec, sep=' ')\n",
    "for word in voca:\n",
    "    enc = sp_model.encode_as_pieces(word)\n",
    "    vector = np.zeros(300)\n",
    "    piece_size = 0\n",
    "    for piece in enc:\n",
    "        if piece in emb_dict.keys():\n",
    "            vector += emb_dict[piece]\n",
    "            piece_size += 1\n",
    "\n",
    "    if piece_size != 0:\n",
    "        vector = vector/piece_size\n",
    "    else:\n",
    "        oov += 1\n",
    "        vector =  np.random.rand(300)\n",
    "    embed_space.append(vector)\n",
    "for word in words:\n",
    "    enc = sp_model.encode_as_pieces(word)\n",
    "    vector = np.zeros(300)\n",
    "    piece_size = 0\n",
    "    for piece in enc:\n",
    "        if piece in emb_dict.keys():\n",
    "            vector += emb_dict[piece]\n",
    "            piece_size += 1\n",
    "\n",
    "    if piece_size != 0:\n",
    "        vector = vector/piece_size\n",
    "    else:\n",
    "        oov += 1\n",
    "        vector =  np.random.rand(300)\n",
    "    vectors.append(vector)\n",
    "_top_k_cosine_similarity(vectors, embed_space, voca, words, top_k=5, name='after_highway')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
